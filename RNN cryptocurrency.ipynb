{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "from collections import deque\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 60\n",
    "future_period_predict = 3\n",
    "ratio_to_predict = \"LTC-USD\"\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "name = f\"{seq_len}-SEQ-{future_period_predict}-PRED-{int(time.time())}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(current, future):\n",
    "    if float(future) > float(current):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def preprocess_df(df):\n",
    "    df = df.drop('future',1)\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if col != \"target\":\n",
    "            df[col] = df[col].pct_change()\n",
    "            df.dropna(inplace=True)\n",
    "            df[col] = preprocessing.scale(df[col].values)\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    sequential_data = []\n",
    "    prev_days = deque(maxlen=seq_len)\n",
    "    \n",
    "    for i in df.values:\n",
    "        prev_days.append([n for n in i[:-1]])\n",
    "        if len(prev_days) == seq_len:\n",
    "            sequential_data.append([np.array(prev_days),i[-1]])\n",
    "    \n",
    "    random.shuffle(sequential_data)\n",
    "    \n",
    "    buys = []\n",
    "    sells = []\n",
    "    \n",
    "    for seq, target in sequential_data:\n",
    "        if target == 0:\n",
    "            sells.append([seq,target])\n",
    "        elif target == 1:\n",
    "            buys.append([seq,target])\n",
    "    \n",
    "    random.shuffle(buys)\n",
    "    random.shuffle(sells)\n",
    "    \n",
    "    lower = min(len(buys),len(sells))\n",
    "    \n",
    "    buys = buys[:lower]\n",
    "    sells = sells[:lower]\n",
    "    \n",
    "    sequential_data = buys+sells\n",
    "    random.shuffle(sequential_data)\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    for seq, target in sequential_data:\n",
    "        X.append(seq)\n",
    "        y.append(target)\n",
    "    \n",
    "    return np.array(X),np.array(y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = pd.DataFrame()\n",
    "ratios = [\"BTC-USD\",\"LTC-USD\",\"ETH-USD\",\"BCH-USD\"]\n",
    "for ratio in ratios:\n",
    "    dataset = f\"crypto_data/{ratio}.csv\"\n",
    "    df = pd.read_csv(dataset, names = [\"time\",\"low\",\"high\", \"open\", \"close\", \"volume\"])\n",
    "    df.rename(columns={\"close\":f\"{ratio}_close\", \"volume\":f\"{ratio}_volume\"},inplace=True)\n",
    "    \n",
    "    df.set_index(\"time\",inplace=True)\n",
    "    df = df[[f\"{ratio}_close\",f\"{ratio}_volume\"]]\n",
    "    \n",
    "    if len(main_df) == 0:\n",
    "        main_df = df\n",
    "    else:\n",
    "        main_df = main_df.join(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            LTC-USD_close     future  target\n",
      "time                                        \n",
      "1528968660      96.580002  96.500000       0\n",
      "1528968720      96.660004  96.389999       0\n",
      "1528968780      96.570000  96.519997       0\n",
      "1528968840      96.500000  96.440002       0\n",
      "1528968900      96.389999  96.470001       1\n",
      "1528968960      96.519997  96.400002       0\n",
      "1528969020      96.440002  96.400002       0\n",
      "1528969080      96.470001  96.400002       0\n",
      "1528969140      96.400002  96.400002       0\n",
      "1528969200      96.400002  96.400002       0\n"
     ]
    }
   ],
   "source": [
    "main_df['future'] = main_df[f\"{ratio_to_predict}_close\"].shift(-future_period_predict)\n",
    "main_df[\"target\"] = list(map(classify,main_df[f\"{ratio_to_predict}_close\"], main_df[\"future\"]))\n",
    "print(main_df[[f\"{ratio_to_predict}_close\",\"future\",\"target\"]].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = sorted(main_df.index.values)\n",
    "last_5pct = times[-int(0.05*len(times))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_main_df = main_df[(main_df.index >= last_5pct)]\n",
    "main_df = main_df[(main_df.index < last_5pct)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = preprocess_df(main_df)\n",
    "validation_x, validation_y = preprocess_df(validation_main_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: 69188, validation: 3062\n",
      "Dont buys: 34594, buys:34594\n",
      "Validation Dont buys: 1531, buys:1531\n"
     ]
    }
   ],
   "source": [
    "print(f\"train data: {len(train_x)}, validation: {len(validation_x)}\")\n",
    "print(f\"Dont buys: {train_y.count(0)}, buys:{train_y.count(1)}\")\n",
    "print(f\"Validation Dont buys: {validation_y.count(0)}, buys:{validation_y.count(1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(train_x.shape[1:]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LSTM(128, input_shape=(train_x.shape[1:]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LSTM(128, input_shape=(train_x.shape[1:])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(32,activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(lr=0.001,decay=1e-6)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tboard_log_dir = os.path.join(\"logs\",name)\n",
    "tensorboard = TensorBoard(log_dir=tboard_log_dir)\n",
    "\n",
    "#filepath = \"RNN_Final-{epoch:02d}-{val_acc:.3f}\"\n",
    "#checkpoint = ModelCheckpoint(\"models\\{}.model\".format(filepath,monitor='val_acc',verbose=1, save_best_only=True,mode='max'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 65962 samples, validate on 3174 samples\n",
      "Epoch 1/10\n",
      "65962/65962 [==============================] - 314s 5ms/sample - loss: 0.7117 - accuracy: 0.5190 - val_loss: 0.6864 - val_accuracy: 0.5583\n",
      "Epoch 2/10\n",
      "65962/65962 [==============================] - 304s 5ms/sample - loss: 0.6892 - accuracy: 0.5368 - val_loss: 0.6872 - val_accuracy: 0.5495\n",
      "Epoch 3/10\n",
      "65962/65962 [==============================] - 310s 5ms/sample - loss: 0.6848 - accuracy: 0.5536 - val_loss: 0.6826 - val_accuracy: 0.5712\n",
      "Epoch 4/10\n",
      "65962/65962 [==============================] - 307s 5ms/sample - loss: 0.6817 - accuracy: 0.5646 - val_loss: 0.6800 - val_accuracy: 0.5555\n",
      "Epoch 5/10\n",
      "65962/65962 [==============================] - 322s 5ms/sample - loss: 0.6796 - accuracy: 0.5687 - val_loss: 0.6785 - val_accuracy: 0.5740\n",
      "Epoch 6/10\n",
      "65962/65962 [==============================] - 306s 5ms/sample - loss: 0.6781 - accuracy: 0.5723 - val_loss: 0.6850 - val_accuracy: 0.5536\n",
      "Epoch 7/10\n",
      "65962/65962 [==============================] - 308s 5ms/sample - loss: 0.6762 - accuracy: 0.5759 - val_loss: 0.6749 - val_accuracy: 0.5810\n",
      "Epoch 8/10\n",
      "65962/65962 [==============================] - 312s 5ms/sample - loss: 0.6722 - accuracy: 0.5850 - val_loss: 0.6757 - val_accuracy: 0.5725\n",
      "Epoch 9/10\n",
      "65962/65962 [==============================] - 310s 5ms/sample - loss: 0.6675 - accuracy: 0.5940 - val_loss: 0.6902 - val_accuracy: 0.5491\n",
      "Epoch 10/10\n",
      "65962/65962 [==============================] - 309s 5ms/sample - loss: 0.6612 - accuracy: 0.6044 - val_loss: 0.6876 - val_accuracy: 0.5630\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_x,train_y,batch_size=batch_size,epochs=epochs,\n",
    "                   validation_data=(validation_x,validation_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models\\{}\".format(name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
